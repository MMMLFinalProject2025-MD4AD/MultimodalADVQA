@article{Liang-foundations-2024,
    author = {Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe},
    title = {Foundations \& Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions},
    year = {2024},
    issue_date = {October 2024},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {56},
    number = {10},
    issn = {0360-0300},
    url = {https://doi.org/10.1145/3656580},
    doi = {10.1145/3656580},
    journal = {ACM Comput. Surv.},
    month = jun,
    articleno = {264},
    numpages = {42},
}


@inproceedings{EMNLP:Bisk2020,
    author    = {Yonatan Bisk and Ari Holtzman and Jesse Thomason and Jacob Andreas and Yoshua Bengio and Joyce Chai and Mirella Lapata and Angeliki Lazaridou and Jonathan May and Aleksandr Nisnevich and Nicolas Pinto and Joseph Turian},
    title     = {Experience Grounds Language},
    booktitle = {Conference on Empirical Methods in Natural Language Processing},
    year      = {2020},
    url       = {https://arxiv.org/abs/2004.10151},
}
                  

@inproceedings{fried-pragmatics-2023,
    author = {Daniel Fried and Nicholas Tomlin and Jennifer Hu and Roma Patel and Aida Nematzadeh},
    title = {Pragmatics in Language Grounding: Phenomena, Tasks, and Modeling Approaches},
    booktitle = {Findings of the Conference on Empirical Methods in Natural Language Processing},
    year = {2023},
    url = {https://arxiv.org/abs/2211.08371}
}

@article{qian2024nuscenes,
	author    = {Tianwen Qian and Jingjing Chen and Linhai Zhuo and Yang Jiao and Yu-Gang Jiang},
	title     = {NuScenes-QA: A Multi-Modal Visual Question Answering Benchmark for Autonomous Driving Scenario},
	journal   = {arXiv preprint arXiv:2305.14836},
	year      = {2024},
	url       = {https://github.com/qiantianwen/NuScenes-QA}
}

@article{openemma,
	author = {Xing, Shuo and Qian, Chengyuan and Wang, Yuping and Hua, Hongyuan and Tian, Kexin and Zhou, Yang and Tu, Zhengzhong},
	title = {OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving},
	journal = {arXiv},
	year = {2024},
	month = dec,
	url   = {https://github.com/taco-group/OpenEMMA},
	eprint = {2412.15208},
	doi = {10.48550/arXiv.2412.15208}
}

@inproceedings{huang2021bevdet,
	author    = {Huang, Junjie and Huang, Guan and Zhu, Zheng and Ye, Yun and Du, Dalong},
	title     = {Bevdet: High-performance multi-camera 3d object detection in bird-eye-view},
	booktitle = {arXiv preprint arXiv:2112.11790},
	year      = {2021},
	url       = {https://arxiv.org/abs/2112.11790}
}

@inproceedings{anderson2018bottom,
	author    = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
	title     = {Bottom-up and top-down attention for image captioning and visual question answering},
	booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)},
	year      = {2018},
	url       = {https://arxiv.org/abs/1707.07998}
}

@inproceedings{yin2021centerpoint,
	author    = {Tianwei Yin and Xingyi Zhou and Philipp Kr{\"a}henb{\"u}hl},
	title     = {CenterPoint: A Unified Framework for 3D Object Detection},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	year      = {2021},
	url       = {https://arxiv.org/abs/2006.11275}
}

@inproceedings{yu2019deep,
	author    = {Yu, Zhou and Yu, Jun and Cui, Yuhao and Tao, Dacheng and Tian, Qi},
	title     = {Deep modular co-attention networks for visual question answering},
	booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)},
	year      = {2019},
	url       = {https://arxiv.org/abs/1906.10770}
}

@inproceedings{zhang2022msmdfusion,
	author    = {Jiao, Yang and Jie, Zequn and Chen, Shaoxiang and Chen, Jingjing and Ma, Lin and Jiang, Yu-Gang},
	title     = {Msmdfusion: Fusing lidar and camera at multiple scales with multi-depth seeds for 3d object detection},
	booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)},
	year      = {2023},
	url       = {https://arxiv.org/abs/2209.03102}
}

@article{Ettinger2021LargeSI,
	title={Large Scale Interactive Motion Forecasting for Autonomous Driving : The Waymo Open Motion Dataset},
	author={Scott M. Ettinger and Shuyang Cheng and Benjamin Caine and Chenxi Liu and Hang Zhao and Sabeek Pradhan and Yuning Chai and Benjamin Sapp and C. Qi and Yin Zhou and Zoey Yang and Aurelien Chouard and Pei Sun and Jiquan Ngiam and Vijay Vasudevan and Alexander McCauley and Jonathon Shlens and Drago Anguelov},
	journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
	year={2021},
	pages={9690-9699},
	url={https://api.semanticscholar.org/CorpusID:233307215}
}

@article{Hwang2024EMMAEM,
	title={EMMA: End-to-End Multimodal Model for Autonomous Driving},
	author={Jyh-Jing Hwang and Runsheng Xu and Hubert Lin and Wei-Chih Hung and Jingwei Ji and Kristy Choi and Di Huang and Tong He and Paul Covington and Benjamin Sapp and James Guo and Drago Anguelov and Mingxing Tan},
	journal={ArXiv},
	year={2024},
	volume={abs/2410.23262},
	url={https://api.semanticscholar.org/CorpusID:273695673}
}